{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import re\n",
    "import string\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, auc, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer \n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from textblob import TextBlob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Womens Clothing E-Commerce Reviews - NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/womens-ecommerce-clothing-reviews/Womens Clothing E-Commerce Reviews.csv\", index_col=0)\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Rating', 'Recommended IND'])['Recommended IND'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df.Rating==5) & (df['Recommended IND']==0)]['Review Text'].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The tagging of the target variable is inaccurate, but I'm about to use it as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = df[['Title', 'Review Text', 'Recommended IND']]\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging text features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['Review'] = text_df['Title'] + ' ' + text_df['Review Text']\n",
    "text_df = text_df.drop(labels=['Title','Review Text'] , axis=1)\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.Review.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = text_df[~text_df.Review.isna()]\n",
    "text_df = text_df.rename(columns={\"Recommended IND\": \"Recommended\"})\n",
    "print(\"My data's shape is:\", text_df.shape)\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Value - positive\\negative review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['Recommended'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['Recommended'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The target is imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['Review_length'] = text_df['Review'].apply(len)\n",
    "print(text_df.shape)\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['Review_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11,5)})\n",
    "sns.distplot(text_df['Review_length'] ,hist=True, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zero = text_df[text_df['Recommended']==0]\n",
    "df_one = text_df[text_df['Recommended']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_zero[['Review_length']] ,hist=False)\n",
    "sns.distplot(df_one[['Review_length']], hist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exclamation mark counter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_exclamation_mark(string_text):\n",
    "    count = 0\n",
    "    for char in string_text:\n",
    "        if char == '!':\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['count_exc'] = text_df['Review'].apply(count_exclamation_mark)\n",
    "text_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['count_exc'].describe(np.arange(0.2, 1.0, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['count_exc'].value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df[text_df['count_exc']== 41].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['Review'][3301]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polarity is the emotion expressed in the sentence. It can be positive, neagtive and neutral.\n",
    "\n",
    "The polarity score is a float within the range [-1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['Polarity'] = text_df['Review'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "text_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['Polarity'].plot(kind='hist', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prep = text_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing - text features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation_removal(messy_str):\n",
    "    clean_list = [char for char in messy_str if char not in string.punctuation]\n",
    "    clean_str = ''.join(clean_list)\n",
    "    return clean_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prep['Review'] = text_prep['Review'].apply(punctuation_removal)\n",
    "text_prep['Review'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech filter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mostly adjectives and verbs reflect the positiveness or negativeness of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"http://josecarilloforum.com/imgs/longnounphrase_schematic-1B.png\", width=600, height=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_collector(review_string):\n",
    "    new_string=[]\n",
    "    review_string = word_tokenize(review_string)\n",
    "    tup_word = nltk.pos_tag(review_string)\n",
    "    for tup in tup_word:\n",
    "        if 'VB' in tup[1] or tup[1]=='JJ':  #Verbs and Adjectives\n",
    "            new_string.append(tup[0])  \n",
    "    return ' '.join(new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prep['Review'] = text_prep['Review'].apply(adj_collector)\n",
    "text_prep['Review'].head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words('english')[::12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop.append(\"i'm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "\n",
    "for item in stop: \n",
    "    new_item = punctuation_removal(item)\n",
    "    stop_words.append(new_item) \n",
    "print(stop_words[::12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding clothing stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clothes_list =['dress', 'top','sweater','shirt',\n",
    "               'skirt','material', 'white', 'black',\n",
    "              'jeans', 'fabric', 'color','order', 'wear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_removal(messy_str):\n",
    "    messy_str = word_tokenize(messy_str)\n",
    "    return [word.lower() for word in messy_str \n",
    "            if word.lower() not in stop_words and word.lower() not in clothes_list ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prep['Review'] = text_prep['Review'].apply(stopwords_removal)\n",
    "text_prep['Review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_prep['Review'][3301])\n",
    "\n",
    "#'Beautiful and unique. Love this top, just received it today.\n",
    "# \\nit is a very artistic interpretation for a casual top.\n",
    "# \\nthe blue is gorgeous!\n",
    "# \\nthe unique style of the peplm and the details on the front set this apart!\n",
    "# \\nruns a little shorter, but i feel the length enhances it;s beauty, and is appropriate for the overall design.\n",
    "# \\nlove !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\nordered my usual size and it fits perfectly.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_prep['Review'][267]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing all numbers (weight, size etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_numbers(list_text):\n",
    "    list_text_new = []\n",
    "    for i in list_text:\n",
    "        if not re.search('\\d', i):\n",
    "            list_text_new.append(i)\n",
    "    return ' '.join(list_text_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prep['Review'] = text_prep['Review'].apply(drop_numbers)\n",
    "text_prep['Review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_prep['Review'][267]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_prep['Review'][2293])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prep['Review'] = text_prep['Review'].apply(lambda x: x.split())\n",
    "text_prep['Review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_update(text_list):\n",
    "    text_list_new = []\n",
    "    for word in text_list:\n",
    "        word = porter.stem(word)\n",
    "        text_list_new.append(word) \n",
    "    return text_list_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prep['Review'] = text_prep['Review'].apply(stem_update)\n",
    "text_prep['Review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prep['Review'] = text_prep['Review'].apply(lambda x: ' '.join(x))\n",
    "text_prep['Review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_prep['Review'][2293])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCloud - Repetition of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = text_prep[text_prep.Recommended== 1]\n",
    "neg_df = text_prep[text_prep.Recommended== 0]\n",
    "pos_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words =[]\n",
    "neg_words = []\n",
    "\n",
    "for review in pos_df.Review:\n",
    "    pos_words.append(review) \n",
    "pos_words = ' '.join(pos_words)\n",
    "pos_words[:40]\n",
    "\n",
    "for review in neg_df.Review:\n",
    "    neg_words.append(review)\n",
    "neg_words = ' '.join(neg_words)\n",
    "neg_words[:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(pos_words)\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\",max_words=len(pos_words),\\\n",
    "                      max_font_size=40, relative_scaling=.5, colormap='summer').generate(pos_words)\n",
    "plt.figure(figsize=(13,13))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(neg_words)\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\",max_words=len(neg_words),\\\n",
    "                      max_font_size=40, relative_scaling=.5, colormap='gist_heat').generate(neg_words)\n",
    "plt.figure(figsize=(13,13))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention to the words: return, disappoint, small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing - Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prep['Review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_vectorizing_process(sentence_string):\n",
    "    return [word for word in sentence_string.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_transformer = CountVectorizer(text_vectorizing_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_transformer.fit(text_prep['Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_prep['Review'].iloc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = bow_transformer.transform([text_prep['Review'].iloc[3]])\n",
    "print(example)\n",
    "#3507=Love\n",
    "#4438=petit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reviews = bow_transformer.transform(text_prep['Review'])\n",
    "Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of Sparse Matrix', Reviews.shape)\n",
    "print('Amount of Non-Zero occurences:', Reviews.nnz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Frequencyâ€“Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer().fit(Reviews)\n",
    "\n",
    "tfidf_example = tfidf_transformer.transform(example)\n",
    "print (tfidf_example)\n",
    "#3507=Love\n",
    "#4438=petit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in bow_transformer.vocabulary_.items() if i[1]==3507]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in bow_transformer.vocabulary_.items()][6:60:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_tfidf = tfidf_transformer.transform(Reviews)\n",
    "messages_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(messages_tfidf[:1]) \n",
    "#tuple(index_num, word_num), tfidf_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Sparse matrix with other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_tfidf = messages_tfidf.toarray()\n",
    "messages_tfidf = pd.DataFrame(messages_tfidf)\n",
    "print(messages_tfidf.shape)\n",
    "messages_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.merge(text_prep.drop(columns='Review'),messages_tfidf, \n",
    "                  left_index=True, right_index=True )\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_all.drop('Recommended', axis=1)\n",
    "y = df_all.Recommended\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split(X,y, test_size=0.3, stratify=y, random_state=111)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaler - MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_scaled,columns= X_train.columns).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_transformer = PCA(n_components=2).fit(X_train_scaled)\n",
    "X_train_scaled_pca = pca_transformer.transform(X_train_scaled)\n",
    "X_test_scaled_pca = pca_transformer.transform(X_test_scaled)\n",
    "X_train_scaled_pca[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "sns.scatterplot(x=X_train_scaled_pca[:, 0], \n",
    "                y=X_train_scaled_pca[:, 1], \n",
    "                hue=y_train, \n",
    "                sizes=100,\n",
    "                palette=\"inferno\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scipy.sparse.csr_matrix(X_train_scaled)\n",
    "X_test_scaled = scipy.sparse.csr_matrix(X_test_scaled)\n",
    "\n",
    "X_train = scipy.sparse.csr_matrix(X_train.values)\n",
    "X_test = scipy.sparse.csr_matrix(X_test.values)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(y_true, y_pred, labels):\n",
    "    cm = pd.DataFrame(confusion_matrix(y_true=y_true, y_pred=y_pred), \n",
    "                                        index=labels, columns=labels)\n",
    "    rep = classification_report(y_true=y_true, y_pred=y_pred)\n",
    "    return (f'Confusion Matrix:\\n{cm}\\n\\nClassification Report:\\n{rep}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation will be made by two metrics:\n",
    "1. <u> F1 micro score</u>  - which is the harmonic mean of precision and recall, and takes into account label imbalances.\n",
    "2. <u> AUC </u>- ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = SVC(C=1.0, \n",
    "             kernel='linear',\n",
    "             class_weight='balanced', \n",
    "             probability=True,\n",
    "             random_state=111)\n",
    "svc_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = svc_model.predict(X_test_scaled)\n",
    "print(report(y_test, test_predictions, svc_model.classes_ ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_roc(y_test, svc_model.predict_proba(X_test_scaled)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(class_weight='balanced', \n",
    "                              random_state=111, \n",
    "                              solver='lbfgs',\n",
    "                              C=1.0)\n",
    "\n",
    "gs_lr_model = GridSearchCV(lr_model, \n",
    "                           param_grid={'C': [0.01, 0.1, 0.5, 1.0, 5.0]}, \n",
    "                           cv=5, \n",
    "                           scoring='roc_auc')\n",
    "\n",
    "gs_lr_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_lr_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = gs_lr_model.predict(X_test_scaled)\n",
    "print(report(y_test, test_predictions, gs_lr_model.classes_ ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_roc(y_test, gs_lr_model.predict_proba(X_test_scaled),\n",
    "                      title='ROC Curves - Logistic Regression') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=5, class_weight='balanced', random_state=555)\n",
    "\n",
    "ada_model = AdaBoostClassifier(base_estimator=dt, learning_rate=0.001, n_estimators=1000, random_state=222)\n",
    "ada_model.fit(X_train ,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = ada_model.predict(X_test)\n",
    "print(report(y_test, test_predictions, ada_model.classes_ ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_roc(y_test, ada_model.predict_proba(X_test), \n",
    "                       title='ROC Curves - AdaBoost') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=1000, max_depth=5, \n",
    "                                  class_weight='balanced', random_state=3)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = rf_model.predict(X_test)\n",
    "print(report(y_test, test_predictions, rf_model.classes_ ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_roc(y_test, rf_model.predict_proba(X_test), \n",
    "                       title='ROC Curves - Random Forest') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = list(zip(rf_model.feature_importances_ ,X.columns))\n",
    "my_list.sort(key=lambda tup: tup[0],reverse=True)\n",
    "my_list[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_list = [i for i in bow_transformer.vocabulary_.items()]\n",
    "\n",
    "for i in my_list:\n",
    "    for j in bow_list:\n",
    "        if i[1] == j[1] and i[0]> 0.005:\n",
    "            print(f'Importance: {i[0]:.4f}   Word num: {i[1]}   Word:  { j[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest - Threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = rf_model.predict_proba(X_train)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, probs[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "plt.subplots(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, '-', label=\"ROC curve\")\n",
    "plt.plot(np.linspace(0,1,10), np.linspace(0,1,10), label=\"diagonal\")\n",
    "for x, y, txt in zip(fpr[::100], tpr[::100], thresholds[::100]):\n",
    "    plt.annotate(np.round(txt,3), (x, y-0.03), fontsize='x-small')\n",
    "rnd_idx = 700\n",
    "plt.annotate('this point refers to the tpr and the fpr\\n at a probability threshold of {}'\\\n",
    "             .format(np.round(thresholds[rnd_idx], 4)), \n",
    "             xy=(fpr[rnd_idx], tpr[rnd_idx]), xytext=(fpr[rnd_idx]+0.2, tpr[rnd_idx]-0.25),\n",
    "             arrowprops=dict(facecolor='black', lw=2, arrowstyle='->',color='r'),)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = rf_model.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, probs[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "plt.subplots(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, '-', label=\"ROC curve\")\n",
    "plt.plot(np.linspace(0,1,10), np.linspace(0,1,10), label=\"diagonal\")\n",
    "for x, y, txt in zip(fpr[::70], tpr[::70], thresholds[::70]):\n",
    "    plt.annotate(np.round(txt,4), (x, y-0.01))\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SVM - f1 micro score of 0.72\n",
    "* Logistic Regression - f1 micro score of 0.72\n",
    "* AdaBoost - f1 micro score of 0.75\n",
    "* Random Forest - f1 micro score of 0.76\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you! \n",
    "\n",
    "The code below is for checking the f1 score of the chosen threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train.toarray(), columns=X.columns)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(X_test.toarray(), columns=X.columns)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr= rf_model.predict_proba(X_test)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_list = arr.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_list[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_list = []\n",
    "for i in arr_list:\n",
    "    proba_list.append(i[0])\n",
    "proba_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['Proba0'] = proba_list\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_list = []\n",
    "for i in X_test['Proba0']:\n",
    "    if i > 0.4998:\n",
    "        prediction_list.append(0)\n",
    "    else:\n",
    "        prediction_list.append(1)\n",
    "prediction_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['Predictions'] = prediction_list\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report(y_test, X_test['Predictions'], rf_model.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
